{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12704069,"sourceType":"datasetVersion","datasetId":8029037}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"# ===================== 0) Imports & fixed base path =====================\nimport warnings, numpy as np, pandas as pd, lightgbm as lgb\nfrom pathlib import Path\nfrom sklearn.model_selection import TimeSeriesSplit\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 200)\n\n# >>> set this to your dataset mount <<<\nBASE = Path(\"/kaggle/input/gq-volt\")   # change if your dataset path differs\nSAMPLE_SUB = BASE / \"submission.csv\"\n\nPEER_ASSETS = [\"BTC\",\"SOL\",\"DOGE\",\"DOT\",\"LINK\",\"SHIB\"]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:27:00.951275Z","iopub.execute_input":"2025-08-09T21:27:00.952201Z","iopub.status.idle":"2025-08-09T21:27:00.957796Z","shell.execute_reply.started":"2025-08-09T21:27:00.952169Z","shell.execute_reply":"2025-08-09T21:27:00.956752Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ===================== 1) Helpers: timestamp parser & feature builders =====================\ndef smart_to_datetime(series: pd.Series) -> pd.Series:\n    \"\"\"Parse timestamps robustly: strings or epoch (s/ms) -> UTC datetimes.\"\"\"\n    if pd.api.types.is_datetime64_any_dtype(series):\n        return series.dt.tz_localize('UTC') if series.dt.tz is None else series\n    num = pd.to_numeric(series, errors='coerce')\n    is_num = num.notna()\n    out_num = pd.to_datetime(\n        num.where(is_num, np.nan),\n        unit=('ms' if (is_num.any() and num[is_num].max() > 1e12) else 's'),\n        utc=True, errors='coerce'\n    )\n    out_str = pd.to_datetime(series.where(~is_num, None), utc=True, errors='coerce')\n    return out_str.fillna(out_num)\n\ndef build_eth_features(df: pd.DataFrame, is_test: bool=False, keep_cols=None):\n    df = df.copy()\n    df['timestamp'] = smart_to_datetime(df['timestamp'])\n    df = df.dropna(subset=['timestamp']).sort_values('timestamp')\n    df = df.groupby('timestamp', as_index=False).last()\n\n    # numeric coercion\n    num_cols = [c for c in df.columns if c != 'timestamp']\n    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors='coerce')\n\n    # mid_price guard (safe logs)\n    mp = df['mid_price']\n    mask = mp.gt(0, fill_value=False) & np.isfinite(mp.values)\n    df['mid_price'] = mp.where(mask).ffill().fillna(1e-8)\n\n    for c in ['ask_price1','bid_price1','ask_volume1','bid_volume1',\n              'bid_volume2','bid_volume3','bid_volume4','bid_volume5',\n              'ask_volume2','ask_volume3','ask_volume4','ask_volume5']:\n        if c in df.columns:\n            df[c] = df[c].where(np.isfinite(df[c]), np.nan).fillna(0.0)\n\n    # ---- L1 microstructure ----\n    df['spread']     = (df['ask_price1'] - df['bid_price1']).fillna(0.0)\n    df['rel_spread'] = (df['spread'] / df['mid_price'].replace(0, np.nan)).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n    denom_vol = (df['bid_volume1'] + df['ask_volume1']) + 1e-9\n    df['imb1']       = (df['bid_volume1'] - df['ask_volume1']) / denom_vol\n\n    # microprice (weighted by opposite side size)\n    df['microprice'] = (\n        df['ask_price1'] * df['bid_volume1'] + df['bid_price1'] * df['ask_volume1']\n    ) / denom_vol\n\n    # queue imbalance variants\n    df['qi1'] = (df['bid_volume1'] - df['ask_volume1']) / denom_vol\n\n    # ---- depth across L1–L5 (if present) ----\n    lvls = [i for i in range(1,6) if f'bid_volume{i}' in df.columns and f'ask_volume{i}' in df.columns]\n    if lvls:\n        bcols = [f'bid_volume{i}' for i in lvls]\n        acols = [f'ask_volume{i}' for i in lvls]\n        df['bid_depth'] = df[bcols].sum(axis=1)\n        df['ask_depth'] = df[acols].sum(axis=1)\n        dden = (df['bid_depth'] + df['ask_depth']) + 1e-9\n        df['depth_imb'] = (df['bid_depth'] - df['ask_depth']) / dden\n\n        # queue imbalance over top-5 (sum of volumes)\n        df['qi5'] = (df['bid_depth'] - df['ask_depth']) / dden\n    else:\n        df['bid_depth'] = 0.0\n        df['ask_depth'] = 0.0\n        df['depth_imb'] = 0.0\n        df['qi5']       = 0.0\n\n    # ---- time-based stats ----\n    s = df.set_index('timestamp').sort_index()\n\n    # returns/vols\n    log_mid = np.log(s['mid_price'].astype(float)).replace([np.inf,-np.inf], np.nan)\n    s['log_ret_1s'] = log_mid.diff().fillna(0.0)\n    s['rv10s']      = s['log_ret_1s'].rolling('10s').std().fillna(0.0)\n    s['rv30s']      = s['log_ret_1s'].rolling('30s').std().fillna(0.0)\n    s['rv60s']      = s['log_ret_1s'].rolling('60s').std().fillna(0.0)\n\n    # momentum & smoothing\n    s['ret5s']       = s['mid_price'].pct_change(5, fill_method=None).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n    s['spread_ma10'] = s['spread'].rolling('10s').mean().fillna(0.0)\n    s['spread_ma30'] = s['spread'].rolling('30s').mean().fillna(0.0)\n    s['imb1_ma10']   = s['imb1'].rolling('10s').mean().fillna(0.0)\n    s['imb1_ma30']   = s['imb1'].rolling('30s').mean().fillna(0.0)\n\n    # microprice dynamics\n    s['micro_ret_1s'] = np.log(s['microprice'].astype(float).clip(lower=1e-8)).diff().fillna(0.0)\n\n    df = s.reset_index()\n\n    # ---- feature set ----\n    feature_cols = [\n        'mid_price','microprice','spread','rel_spread',\n        'imb1','qi1','qi5','bid_depth','ask_depth','depth_imb',\n        'log_ret_1s','micro_ret_1s','rv10s','rv30s','rv60s',\n        'ret5s','spread_ma10','spread_ma30','imb1_ma10','imb1_ma30'\n    ]\n\n    if keep_cols is not None:\n        for m in keep_cols:\n            if m not in df.columns:\n                df[m] = 0.0\n        feature_cols = keep_cols\n\n    df[feature_cols] = (\n        df[feature_cols]\n        .apply(pd.to_numeric, errors='coerce')\n        .replace([np.inf, -np.inf], np.nan)\n        .fillna(0.0)\n    )\n\n    if not is_test:\n        if 'label' not in df.columns:\n            raise ValueError(\"Expected 'label' in train/ETH.csv; it's the 10s-ahead IV.\")\n        return df[['timestamp'] + feature_cols + ['label']], feature_cols\n    else:\n        return df[['timestamp'] + feature_cols], feature_cols\n# ---- Extra microstructure / multi-scale features ----\ndef add_more_feats(df):\n    s = df.set_index('timestamp').sort_index()\n\n    # microprice displacement\n    s['micro_disp'] = (s['microprice'] - s['mid_price']) / s['mid_price']\n\n    # range / turbulence\n    s['mid_roll_max_10'] = s['mid_price'].rolling('10s').max()\n    s['mid_roll_min_10'] = s['mid_price'].rolling('10s').min()\n    s['range10']         = (s['mid_roll_max_10'] - s['mid_roll_min_10']) / s['mid_price']\n    s['spread_std10']    = s['spread'].rolling('10s').std()\n\n    # change speed\n    s['d_spread'] = s['spread'].diff().fillna(0.0)\n    s['d_imb1']   = s['imb1'].diff().fillna(0.0)\n    s['d_depth']  = (s['bid_depth'] - s['ask_depth']).diff().fillna(0.0)\n\n    # longer vol windows (HAR-ish)\n    for w in ['120s','300s']:\n        s[f'rv{w}'] = s['log_ret_1s'].rolling(w).std()\n\n    # intraday cycles\n    sec = s.index.view('int64') // 10**9\n    s['sin_1h'] = np.sin(2*np.pi*(sec % 3600)/3600)\n    s['cos_1h'] = np.cos(2*np.pi*(sec % 3600)/3600)\n\n    out = s.reset_index()\n    extra = ['micro_disp','range10','spread_std10','d_spread','d_imb1','d_depth','rv120s','rv300s','sin_1h','cos_1h']\n    for c in extra:\n        if c not in out: out[c] = 0.0\n    return out, extra\n\n\ndef make_lob_feats(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Peer features for proxy pretraining (next-10s realized vol).\"\"\"\n    d = df.copy()\n    d['timestamp'] = smart_to_datetime(d['timestamp'])\n    d = d.dropna(subset=['timestamp']).sort_values('timestamp')\n    d = d.groupby('timestamp', as_index=False).last()\n\n    num_cols = [c for c in d.columns if c != 'timestamp']\n    d[num_cols] = d[num_cols].apply(pd.to_numeric, errors='coerce')\n\n    d['mid']   = d['mid_price']\n    d['spread']= d['ask_price1'] - d['bid_price1']\n    d['rel_spread'] = d['spread'] / d['mid'].replace(0, np.nan)\n    d['imb1']  = (d['bid_volume1'] - d['ask_volume1']) / (d['bid_volume1'] + d['ask_volume1'] + 1e-9)\n\n    s = d.set_index('timestamp').sort_index()\n    ret1 = np.log(s['mid'].astype(float).clip(lower=1e-8)).diff().fillna(0.0)\n    rv10_past = ret1.rolling('10s').std()\n    rv10_future = ret1.rolling('10s').std().shift(-10)\n\n    out = pd.DataFrame({\n        'timestamp': s.index,\n        'mid': s['mid'].values,\n        'spread': s['spread'].values,\n        'rel_spread': s['rel_spread'].values,\n        'imb1': s['imb1'].values,\n        'ret1': ret1.values,\n        'rv10_past': rv10_past.fillna(0.0).values,\n        'rv10_future_proxy': rv10_future.values,\n    })\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:27:00.959407Z","iopub.execute_input":"2025-08-09T21:27:00.959718Z","iopub.status.idle":"2025-08-09T21:27:00.991057Z","shell.execute_reply.started":"2025-08-09T21:27:00.959668Z","shell.execute_reply":"2025-08-09T21:27:00.990053Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ===================== 2) Load ETH supervised & peer proxy data =====================\neth_path = BASE / \"train\" / \"ETH.csv\"\nassert eth_path.exists(), f\"Missing {eth_path}\"\n\ntrain_eth = pd.read_csv(eth_path)\n\n# Build + extend train features\ntrain_fe, base_cols = build_eth_features(train_eth, is_test=False)\ntrain_fe, extra_cols = add_more_feats(train_fe)\nfeature_cols = base_cols + extra_cols\n\nprint(\"ETH feature columns:\", feature_cols)\ntrain_fe.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:27:00.992142Z","iopub.execute_input":"2025-08-09T21:27:00.992599Z","iopub.status.idle":"2025-08-09T21:27:08.995385Z","shell.execute_reply.started":"2025-08-09T21:27:00.992568Z","shell.execute_reply":"2025-08-09T21:27:08.994542Z"}},"outputs":[{"name":"stdout","text":"ETH feature columns: ['mid_price', 'microprice', 'spread', 'rel_spread', 'imb1', 'qi1', 'qi5', 'bid_depth', 'ask_depth', 'depth_imb', 'log_ret_1s', 'micro_ret_1s', 'rv10s', 'rv30s', 'rv60s', 'ret5s', 'spread_ma10', 'spread_ma30', 'imb1_ma10', 'imb1_ma30', 'micro_disp', 'range10', 'spread_std10', 'd_spread', 'd_imb1', 'd_depth', 'rv120s', 'rv300s', 'sin_1h', 'cos_1h']\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                  timestamp  mid_price   microprice  spread  rel_spread  \\\n0 2024-09-25 18:13:28+00:00   2581.605  2581.608979    0.01    0.000004   \n1 2024-09-25 18:13:29+00:00   2581.285  2581.286344    0.01    0.000004   \n2 2024-09-25 18:13:30+00:00   2581.285  2581.285201    0.01    0.000004   \n3 2024-09-25 18:13:31+00:00   2581.105  2581.107038    0.01    0.000004   \n4 2024-09-25 18:13:32+00:00   2581.105  2581.107356    0.01    0.000004   \n\n       imb1       qi1       qi5  bid_depth  ask_depth  depth_imb  log_ret_1s  \\\n0  0.795740  0.795740  0.800225     3361.2      373.0   0.800225    0.000000   \n1  0.268774  0.268774  0.121436     1533.9     1201.7   0.121436   -0.000124   \n2  0.040233  0.040233 -0.107131      957.2     1186.9  -0.107131    0.000000   \n3  0.407600  0.407600  0.320310     2510.9     1292.6   0.320310   -0.000070   \n4  0.471174  0.471174  0.347953     2526.4     1222.1   0.347953    0.000000   \n\n   micro_ret_1s     rv10s     rv30s     rv60s  ret5s  spread_ma10  \\\n0  0.000000e+00  0.000000  0.000000  0.000000    0.0         0.01   \n1 -1.249821e-04  0.000088  0.000088  0.000088    0.0         0.01   \n2 -4.426892e-07  0.000072  0.000072  0.000072    0.0         0.01   \n3 -6.902349e-05  0.000060  0.000060  0.000060    0.0         0.01   \n4  1.231527e-07  0.000056  0.000056  0.000056    0.0         0.01   \n\n   spread_ma30  imb1_ma10  imb1_ma30     label    micro_disp  mid_roll_max_10  \\\n0         0.01   0.795740   0.795740  0.000060  1.541173e-06         2581.605   \n1         0.01   0.532257   0.532257  0.000057  5.206208e-07         2581.605   \n2         0.01   0.368249   0.368249  0.000080  7.793144e-08         2581.605   \n3         0.01   0.378087   0.378087  0.000087  7.895832e-07         2581.605   \n4         0.01   0.396704   0.396704  0.000090  9.127360e-07         2581.605   \n\n   mid_roll_min_10   range10  spread_std10      d_spread    d_imb1  d_depth  \\\n0         2581.605  0.000000           NaN  0.000000e+00  0.000000      0.0   \n1         2581.285  0.000124  3.215549e-13 -4.547474e-13 -0.526966  -2656.0   \n2         2581.285  0.000124  2.625484e-13  0.000000e+00 -0.228541   -561.9   \n3         2581.105  0.000194  2.625485e-13  4.547474e-13  0.367367   1448.0   \n4         2581.105  0.000194  2.490755e-13  0.000000e+00  0.063574     86.0   \n\n     rv120s    rv300s    sin_1h    cos_1h  \n0       NaN       NaN  0.987136  0.159881  \n1  0.000088  0.000088  0.987414  0.158158  \n2  0.000072  0.000072  0.987688  0.156434  \n3  0.000060  0.000060  0.987960  0.154710  \n4  0.000056  0.000056  0.988228  0.152986  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>mid_price</th>\n      <th>microprice</th>\n      <th>spread</th>\n      <th>rel_spread</th>\n      <th>imb1</th>\n      <th>qi1</th>\n      <th>qi5</th>\n      <th>bid_depth</th>\n      <th>ask_depth</th>\n      <th>depth_imb</th>\n      <th>log_ret_1s</th>\n      <th>micro_ret_1s</th>\n      <th>rv10s</th>\n      <th>rv30s</th>\n      <th>rv60s</th>\n      <th>ret5s</th>\n      <th>spread_ma10</th>\n      <th>spread_ma30</th>\n      <th>imb1_ma10</th>\n      <th>imb1_ma30</th>\n      <th>label</th>\n      <th>micro_disp</th>\n      <th>mid_roll_max_10</th>\n      <th>mid_roll_min_10</th>\n      <th>range10</th>\n      <th>spread_std10</th>\n      <th>d_spread</th>\n      <th>d_imb1</th>\n      <th>d_depth</th>\n      <th>rv120s</th>\n      <th>rv300s</th>\n      <th>sin_1h</th>\n      <th>cos_1h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024-09-25 18:13:28+00:00</td>\n      <td>2581.605</td>\n      <td>2581.608979</td>\n      <td>0.01</td>\n      <td>0.000004</td>\n      <td>0.795740</td>\n      <td>0.795740</td>\n      <td>0.800225</td>\n      <td>3361.2</td>\n      <td>373.0</td>\n      <td>0.800225</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.795740</td>\n      <td>0.795740</td>\n      <td>0.000060</td>\n      <td>1.541173e-06</td>\n      <td>2581.605</td>\n      <td>2581.605</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.987136</td>\n      <td>0.159881</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024-09-25 18:13:29+00:00</td>\n      <td>2581.285</td>\n      <td>2581.286344</td>\n      <td>0.01</td>\n      <td>0.000004</td>\n      <td>0.268774</td>\n      <td>0.268774</td>\n      <td>0.121436</td>\n      <td>1533.9</td>\n      <td>1201.7</td>\n      <td>0.121436</td>\n      <td>-0.000124</td>\n      <td>-1.249821e-04</td>\n      <td>0.000088</td>\n      <td>0.000088</td>\n      <td>0.000088</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.532257</td>\n      <td>0.532257</td>\n      <td>0.000057</td>\n      <td>5.206208e-07</td>\n      <td>2581.605</td>\n      <td>2581.285</td>\n      <td>0.000124</td>\n      <td>3.215549e-13</td>\n      <td>-4.547474e-13</td>\n      <td>-0.526966</td>\n      <td>-2656.0</td>\n      <td>0.000088</td>\n      <td>0.000088</td>\n      <td>0.987414</td>\n      <td>0.158158</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024-09-25 18:13:30+00:00</td>\n      <td>2581.285</td>\n      <td>2581.285201</td>\n      <td>0.01</td>\n      <td>0.000004</td>\n      <td>0.040233</td>\n      <td>0.040233</td>\n      <td>-0.107131</td>\n      <td>957.2</td>\n      <td>1186.9</td>\n      <td>-0.107131</td>\n      <td>0.000000</td>\n      <td>-4.426892e-07</td>\n      <td>0.000072</td>\n      <td>0.000072</td>\n      <td>0.000072</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.368249</td>\n      <td>0.368249</td>\n      <td>0.000080</td>\n      <td>7.793144e-08</td>\n      <td>2581.605</td>\n      <td>2581.285</td>\n      <td>0.000124</td>\n      <td>2.625484e-13</td>\n      <td>0.000000e+00</td>\n      <td>-0.228541</td>\n      <td>-561.9</td>\n      <td>0.000072</td>\n      <td>0.000072</td>\n      <td>0.987688</td>\n      <td>0.156434</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024-09-25 18:13:31+00:00</td>\n      <td>2581.105</td>\n      <td>2581.107038</td>\n      <td>0.01</td>\n      <td>0.000004</td>\n      <td>0.407600</td>\n      <td>0.407600</td>\n      <td>0.320310</td>\n      <td>2510.9</td>\n      <td>1292.6</td>\n      <td>0.320310</td>\n      <td>-0.000070</td>\n      <td>-6.902349e-05</td>\n      <td>0.000060</td>\n      <td>0.000060</td>\n      <td>0.000060</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.378087</td>\n      <td>0.378087</td>\n      <td>0.000087</td>\n      <td>7.895832e-07</td>\n      <td>2581.605</td>\n      <td>2581.105</td>\n      <td>0.000194</td>\n      <td>2.625485e-13</td>\n      <td>4.547474e-13</td>\n      <td>0.367367</td>\n      <td>1448.0</td>\n      <td>0.000060</td>\n      <td>0.000060</td>\n      <td>0.987960</td>\n      <td>0.154710</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024-09-25 18:13:32+00:00</td>\n      <td>2581.105</td>\n      <td>2581.107356</td>\n      <td>0.01</td>\n      <td>0.000004</td>\n      <td>0.471174</td>\n      <td>0.471174</td>\n      <td>0.347953</td>\n      <td>2526.4</td>\n      <td>1222.1</td>\n      <td>0.347953</td>\n      <td>0.000000</td>\n      <td>1.231527e-07</td>\n      <td>0.000056</td>\n      <td>0.000056</td>\n      <td>0.000056</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.396704</td>\n      <td>0.396704</td>\n      <td>0.000090</td>\n      <td>9.127360e-07</td>\n      <td>2581.605</td>\n      <td>2581.105</td>\n      <td>0.000194</td>\n      <td>2.490755e-13</td>\n      <td>0.000000e+00</td>\n      <td>0.063574</td>\n      <td>86.0</td>\n      <td>0.000056</td>\n      <td>0.000056</td>\n      <td>0.988228</td>\n      <td>0.152986</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# ===================== 3) Build peer proxy pretraining set (optional) =====================\nparts = []\nfor a in PEER_ASSETS:\n    p = BASE / \"train\" / f\"{a}.csv\"\n    if not p.exists():\n        continue\n    raw = pd.read_csv(p)\n    f = make_lob_feats(raw).dropna(subset=['rv10_future_proxy'])\n    parts.append(f[['mid','spread','rel_spread','imb1','ret1','rv10_past','rv10_future_proxy']])\n\nif parts:\n    pre_df = pd.concat(parts, ignore_index=True).replace([np.inf,-np.inf], np.nan).dropna()\n    X_pre = pre_df[['mid','spread','rel_spread','imb1','ret1','rv10_past']].values.astype(float)\n    y_pre = pre_df['rv10_future_proxy'].values.astype(float)\n    print(\"Pretraining samples:\", X_pre.shape[0])\nelse:\n    X_pre = y_pre = None\n    print(\"No peer datasets found; skipping pretraining.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:27:08.997005Z","iopub.execute_input":"2025-08-09T21:27:08.997265Z","iopub.status.idle":"2025-08-09T21:27:26.581768Z","shell.execute_reply.started":"2025-08-09T21:27:08.997246Z","shell.execute_reply":"2025-08-09T21:27:26.580386Z"}},"outputs":[{"name":"stdout","text":"Pretraining samples: 0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ===================== 4) Train with walk-forward CV (Pearson) + final fit =====================\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# --- Purged walk-forward folds (prevents leakage around 10s horizon) ---\ndef purged_folds(n, n_splits=5, gap=20):\n    import numpy as np\n    fold_sizes = np.full(n_splits, n // n_splits, dtype=int)\n    fold_sizes[: n % n_splits] += 1\n    idx = np.arange(n)\n    start = 0\n    for _ in range(n_splits):\n        stop = start + fold_sizes[_]\n        val_idx = idx[start:stop]\n        tr_left  = idx[: max(0, start - gap)]\n        tr_right = idx[min(n, stop + gap):]\n        tr_idx = np.concatenate([tr_left, tr_right])\n        yield tr_idx, val_idx\n        start = stop\n\n# sklearn-style Pearson for LightGBM\ndef lgb_pearson(y_true, y_pred):\n    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n    if y_true.std()<1e-12 or y_pred.std()<1e-12: return ('pearson', 0.0, True)\n    return ('pearson', float(np.corrcoef(y_pred, y_true)[0,1]), True)\n\n\n\n# ===================== 4) Train with PURGED walk-forward CV (Pearson) + final fit =====================\n# (Assumes: build_eth_features done; lgb_pearson() and purged_folds() already defined above)\n\ndef pearson_np(y_true, y_pred):\n    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n    if y_true.std() < 1e-12 or y_pred.std() < 1e-12: return 0.0\n    return float(np.corrcoef(y_pred, y_true)[0, 1])\n\n# Prepare X, y\nX = train_fe[feature_cols].values.astype('float32')\ny = train_fe['label'].astype('float32').values\nrow_ok = np.isfinite(y) & np.isfinite(X).all(axis=1)\nX, y = X[row_ok], y[row_ok]\n\n# Optional pretraining on pre_features (if you built them)\npre_booster = None\nif 'X_pre' in globals() and X_pre is not None and len(X_pre) > 0:\n    pre_model = lgb.LGBMRegressor(\n        n_estimators=1500, learning_rate=0.05, num_leaves=64,\n        subsample=0.9, colsample_bytree=0.9, random_state=42, verbosity=-1, metric=\"l2\"\n    )\n    pre_model.fit(\n        X_pre, y_pre,\n        eval_set=[(X_pre, y_pre)],\n        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(period=0)]\n    )\n    pre_booster = pre_model.booster_\n    print(\"Pretraining done.\")\n\n# ---- PURGED walk-forward CV (gap ~ horizon*2 to avoid leakage) ----\nfold_corrs, best_iters = [], []\nlast_fold = None\nlast_best_iter = None\n\nfor k, (tr_idx, va_idx) in enumerate(purged_folds(len(X), n_splits=5, gap=20), 1):\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=4000, learning_rate=0.03, num_leaves=64,\n        subsample=0.9, colsample_bytree=0.9, random_state=42,\n        verbosity=-1, metric=None\n    )\n    init_kw = {'init_model': pre_booster} if pre_booster is not None else {}\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=lgb_pearson,\n        callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(period=0)],\n        **init_kw\n    )\n    y_hat = model.predict(X_va, num_iteration=model.best_iteration_)\n    r = pearson_np(y_va, y_hat)\n    fold_corrs.append(r)\n    best_iters.append(model.best_iteration_ or 100)\n    last_fold = (tr_idx, va_idx)          # save last fold for blending/smoothing later\n    last_best_iter = model.best_iteration_\n    print(f\"Fold {k}: Pearson r = {r:.4f} | best_iter = {model.best_iteration_}\")\n\nprint(f\"\\nMean CV Pearson r: {np.mean(fold_corrs):.4f} | Std: {np.std(fold_corrs):.4f}\")\n\nbest_iter_avg  = int(np.clip(np.mean(best_iters), 100, 4000))\nbest_iter_last = int(last_best_iter or best_iter_avg)\nbest_iter_final = best_iter_last          # prefer last fold (closest to test)\nprint(f\"Using best_iter_avg = {best_iter_avg} | best_iter_last = {best_iter_last} -> final = {best_iter_final}\")\n\n# ---- Final fit on all ETH (store for quick single-model inference) ----\nfinal_model = lgb.LGBMRegressor(\n    n_estimators=best_iter_final, learning_rate=0.03, num_leaves=64,\n    subsample=0.9, colsample_bytree=0.9, random_state=42, verbosity=-1, metric=None\n)\ninit_kw = {'init_model': pre_booster} if pre_booster is not None else {}\nfinal_model.fit(X, y, eval_set=[(X, y)], eval_metric=lgb_pearson, callbacks=[lgb.log_evaluation(period=0)], **init_kw)\n\nprint(\"In-sample Pearson (sanity):\", pearson_np(y, final_model.predict(X)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:27:26.582826Z","iopub.execute_input":"2025-08-09T21:27:26.583097Z","iopub.status.idle":"2025-08-09T21:28:52.636149Z","shell.execute_reply.started":"2025-08-09T21:27:26.583076Z","shell.execute_reply":"2025-08-09T21:28:52.635174Z"}},"outputs":[{"name":"stdout","text":"Fold 1: Pearson r = 0.6201 | best_iter = 112\nFold 2: Pearson r = 0.6741 | best_iter = 179\nFold 3: Pearson r = 0.6332 | best_iter = 206\nFold 4: Pearson r = 0.7122 | best_iter = 155\nFold 5: Pearson r = 0.6582 | best_iter = 18\n\nMean CV Pearson r: 0.6596 | Std: 0.0323\nUsing best_iter_avg = 134 | best_iter_last = 18 -> final = 18\nIn-sample Pearson (sanity): 0.7522000526539802\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# === Pick final n_estimators on last fold (avoid underfitting at 18) ===\ntr_idx, va_idx = last_fold  # saved in the CV cell\nX_tr, X_va = X[tr_idx], X[va_idx]\ny_tr, y_va = y[tr_idx], y[va_idx]\n\ncands = sorted(set([\n    int(last_best_iter or 100),\n    int(np.median(best_iters)),\n    int(np.mean(best_iters)),\n    int(1.25 * np.mean(best_iters)),\n]))\n# clamp to a sane range\ncands = [int(np.clip(c, 50, 800)) for c in cands]\n\ndef try_rounds(rounds):\n    scores = []\n    for r in rounds:\n        m = lgb.LGBMRegressor(\n            n_estimators=r, learning_rate=0.03, num_leaves=64,\n            subsample=0.9, colsample_bytree=0.9, random_state=2025,\n            verbosity=-1, metric=None\n        )\n        m.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=lgb_pearson,\n              callbacks=[lgb.log_evaluation(period=0)])\n        p = m.predict(X_va)\n        scores.append((r, float(np.corrcoef(p, y_va)[0,1])))\n    return scores\n\nround_scores = try_rounds(cands)\nround_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:28:52.637258Z","iopub.execute_input":"2025-08-09T21:28:52.637632Z","iopub.status.idle":"2025-08-09T21:29:27.869564Z","shell.execute_reply.started":"2025-08-09T21:28:52.637607Z","shell.execute_reply":"2025-08-09T21:29:27.868656Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[(50, 0.6213304234275834),\n (134, 0.5830988637136082),\n (155, 0.5799279791798324),\n (167, 0.5798513313502405)]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"best_iter_final = max(round_scores, key=lambda x: x[1])[0]\nprint(\"Chosen final n_estimators:\", best_iter_final, \"| candidates tested:\", cands)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:29:27.870571Z","iopub.execute_input":"2025-08-09T21:29:27.870857Z","iopub.status.idle":"2025-08-09T21:29:27.876903Z","shell.execute_reply.started":"2025-08-09T21:29:27.870838Z","shell.execute_reply":"2025-08-09T21:29:27.875927Z"}},"outputs":[{"name":"stdout","text":"Chosen final n_estimators: 50 | candidates tested: [50, 134, 155, 167]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# === Train two decorrelated configs and bag them ===\nCFG_A = dict(learning_rate=0.03, num_leaves=64,  min_child_samples=80,\n             subsample=0.9,  colsample_bytree=0.9,  lambda_l2=2.0)\nCFG_B = dict(learning_rate=0.02, num_leaves=128, min_child_samples=160,\n             subsample=0.85, colsample_bytree=0.85, lambda_l2=8.0)\nSEEDS = [29, 42, 73]  # bump to 5 if you have time\n\ndef fit_bag(X, y, cfg, seeds, n_estimators):\n    bags = []\n    for sd in seeds:\n        m = lgb.LGBMRegressor(metric=None, n_estimators=n_estimators, random_state=sd, verbosity=-1, **cfg)\n        m.fit(X, y, eval_set=[(X, y)], eval_metric=lgb_pearson, callbacks=[lgb.log_evaluation(period=0)])\n        bags.append(m)\n    return bags\n\ndef predict_bags(models, X):\n    return np.column_stack([m.predict(X) for m in models]).mean(axis=1)\n\nbags_A = fit_bag(X, y, CFG_A, SEEDS, best_iter_final)\nbags_B = fit_bag(X, y, CFG_B, SEEDS, best_iter_final)\n\n# --- Tune blend and smoothing on last fold (closest to test) ---\nXv, yv = X[va_idx], y[va_idx]\npA_v = predict_bags(bags_A, Xv)\npB_v = predict_bags(bags_B, Xv)\n\nbest_r, best_w = -1.0, 0.5\nfor w in np.linspace(0.2, 0.8, 13):\n    r = float(np.corrcoef(w*pA_v + (1-w)*pB_v, yv)[0,1])\n    if r > best_r: best_r, best_w = r, w\nprint(\"Blend weight for A:\", best_w, \"| last-fold r:\", best_r)\n# ===================== Dynamic blend by regime (last-fold tuning) =====================\n# pick a regime feature present in both train & test\nreg_name = 'rv30s' if 'rv30s' in train_fe.columns else ('rv10s' if 'rv10s' in train_fe.columns else None)\n\nif reg_name is None:\n    # fallback: no regime split\n    w_low = w_high = float(best_w)\n    thr = None\n    print(\"No rv30s/rv10s in features; using global blend only.\")\nelse:\n    reg_val = train_fe.iloc[va_idx][reg_name].to_numpy(dtype=float)\n    thr = np.nanmedian(reg_val)\n\n    low_mask  = np.isfinite(reg_val) & (reg_val <= thr)\n    high_mask = np.isfinite(reg_val) & ~low_mask\n\n    def best_weight_for(mask):\n        if mask.sum() < 100:   # avoid tiny slices\n            return float(best_w)\n        w_grid = np.linspace(0.2, 0.85, 14)  # 0.2..0.85 step 0.05\n        r_best, w_best = -1.0, float(best_w)\n        for w in w_grid:\n            r = float(np.corrcoef(w*pA_v[mask] + (1-w)*pB_v[mask], yv[mask])[0,1])\n            if r > r_best:\n                r_best, w_best = r, float(w)\n        return w_best\n\n    w_low  = best_weight_for(low_mask)\n    w_high = best_weight_for(high_mask)\n\n    print(f\"Regime weights — calm({reg_name}≤thr): {w_low:.3f}, choppy({reg_name}>thr): {w_high:.3f}, thr={thr:.3e}\")\n\n\nimport pandas as pd\ndef pick_smoothing(y_true, y_pred):\n    best = (float(np.corrcoef(y_pred, y_true)[0,1]), 1)\n    for w in [3,5,7,9,11,15]:\n        sm = pd.Series(y_pred).rolling(w, min_periods=1).mean().values\n        r  = float(np.corrcoef(sm, y_true)[0,1])\n        if r > best[0]: best = (r, w)\n    return best[1]\n\nw_opt = pick_smoothing(yv, best_w*pA_v + (1-best_w)*pB_v)\nprint(\"Chosen smoothing window:\", w_opt)\n\n# ===================== Inference (native cadence → interpolate to sample length) =====================\nss = pd.read_csv(SAMPLE_SUB)  # keep Kaggle's order/length\n\n# Build test features at native cadence (fast)\ntest_eth = pd.read_csv(BASE / \"test\" / \"ETH.csv\")\ntest_eth['timestamp'] = pd.to_datetime(test_eth['timestamp'], utc=True)\ntest_eth = (test_eth.sort_values('timestamp')\n                    .drop_duplicates(subset='timestamp', keep='last'))\n\ntest_fe, _ = build_eth_features(test_eth, is_test=True, keep_cols=feature_cols)\ntest_fe, _ = add_more_feats(test_fe)   # ensure this matches what you did on train\n\n# make sure all train features exist\nfor c in feature_cols:\n    if c not in test_fe.columns:\n        test_fe[c] = 0.0\n\nX_test_native = test_fe[feature_cols].astype('float32').values\n\n# predictions from your bagged models\npA_native = predict_bags(bags_A, X_test_native)\npB_native = predict_bags(bags_B, X_test_native)\n\n# ---- dynamic regime blend on TEST ----\nif ('reg_name' in globals()) and (reg_name is not None) and (reg_name in test_fe.columns) and (thr is not None):\n    reg_test = test_fe[reg_name].to_numpy(dtype=float)\n    mask_low  = np.isfinite(reg_test) & (reg_test <= thr)\n    mask_high = np.isfinite(reg_test) & ~mask_low\n\n    preds_native = np.empty_like(pA_native, dtype=float)\n    preds_native[mask_low]  = w_low  * pA_native[mask_low]  + (1 - w_low)  * pB_native[mask_low]\n    preds_native[mask_high] = w_high * pA_native[mask_high] + (1 - w_high) * pB_native[mask_high]\nelse:\n    # fallback to global blend\n    preds_native = float(best_w) * pA_native + (1 - float(best_w)) * pB_native\n\n# (optional) smoothing — your tuned w_opt=1 means no smoothing needed\n\n# ---- interpolate to EXACT sample length by position (no merge on timestamp) ----\nx_src = np.linspace(0, 1, num=len(preds_native))\nx_tgt = np.linspace(0, 1, num=len(ss))\nss_out = ss.copy()\nss_out['labels'] = np.interp(x_tgt, x_src, preds_native)\n\n# save\nout_path = \"/kaggle/working/submission.csv\"\nss_out.to_csv(out_path, index=False)\nprint(\"Saved:\", out_path, \"| rows:\", len(ss_out), \"| NaNs:\", ss_out['labels'].isna().sum(), \"| zeros:\", (ss_out['labels']==0).sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:29:27.879032Z","iopub.execute_input":"2025-08-09T21:29:27.879301Z","iopub.status.idle":"2025-08-09T21:30:07.933325Z","shell.execute_reply.started":"2025-08-09T21:29:27.879280Z","shell.execute_reply":"2025-08-09T21:30:07.932468Z"}},"outputs":[{"name":"stdout","text":"Blend weight for A: 0.7500000000000002 | last-fold r: 0.7299311148945875\nRegime weights — calm(rv30s≤thr): 0.850, choppy(rv30s>thr): 0.200, thr=9.533e-05\nChosen smoothing window: 1\nSaved: /kaggle/working/submission.csv | rows: 270548 | NaNs: 0 | zeros: 0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# === Run metadata + submission sanity ===\nimport json, platform, sys\nfrom pathlib import Path\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n\n# Collect metadata (best effort; fills None if var not in scope)\nmeta = {\n    \"cv_mean\": float(np.mean(fold_corrs)) if 'fold_corrs' in globals() else None,\n    \"cv_std\": float(np.std(fold_corrs)) if 'fold_corrs' in globals() else None,\n    \"last_fold_r\": float(best_r) if 'best_r' in globals() else None,\n    \"best_iter_A\": int(best_iter_A) if 'best_iter_A' in globals() else None,\n    \"best_iter_B\": int(best_iter_B) if 'best_iter_B' in globals() else None,\n    \"seeds\": list(SEEDS) if 'SEEDS' in globals() else None,\n    \"blend_w_global\": float(best_w) if 'best_w' in globals() else None,\n    \"regime_feature\": reg_name if 'reg_name' in globals() else None,\n    \"regime_threshold\": float(thr) if 'thr' in globals() and thr is not None else None,\n    \"w_low\": float(w_low) if 'w_low' in globals() else None,\n    \"w_high\": float(w_high) if 'w_high' in globals() else None,\n    \"smoothing_window\": int(w_opt) if 'w_opt' in globals() else 1,\n    \"n_features\": int(len(feature_cols)) if 'feature_cols' in globals() else None,\n    \"python\": sys.version.split()[0],\n    \"platform\": platform.platform(),\n}\n\nout_json = Path(\"/kaggle/working/run_config.json\")\nout_json.write_text(json.dumps(meta, indent=2))\nprint(\"Saved run_config.json\")\nprint(json.dumps(meta, indent=2))\n\n# Load submission and sanity check\nsub_path = Path(\"/kaggle/working/submission.csv\")\nassert sub_path.exists(), f\"Missing {sub_path}\"\ns = pd.read_csv(sub_path)\n\nprint(\"\\nSubmission:\")\nprint(\"shape:\", s.shape, \"| columns:\", s.columns.tolist())\nprint(\"NaNs:\", int(s.isna().sum().sum()), \"| zeros:\", int((s['labels']==0).sum()))\nprint(\"stats: min/mean/median/max/std =\",\n      float(s['labels'].min()),\n      float(s['labels'].mean()),\n      float(s['labels'].median()),\n      float(s['labels'].max()),\n      float(s['labels'].std()))\nprint(\"quantiles:\", s['labels'].quantile([0.01,0.25,0.5,0.75,0.99]).to_dict())\n\n# Quick histogram (single plot, default style per rules)\nplt.figure(figsize=(9,4))\nplt.hist(s['labels'].values, bins=100)\nplt.title(\"Submission label distribution\")\nplt.xlabel(\"labels\"); plt.ylabel(\"count\")\nplt.tight_layout(); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T21:31:25.266512Z","iopub.execute_input":"2025-08-09T21:31:25.266855Z","iopub.status.idle":"2025-08-09T21:31:25.892352Z","shell.execute_reply.started":"2025-08-09T21:31:25.266834Z","shell.execute_reply":"2025-08-09T21:31:25.891364Z"}},"outputs":[{"name":"stdout","text":"Saved run_config.json\n{\n  \"cv_mean\": 0.6595622011774103,\n  \"cv_std\": 0.03234031913071235,\n  \"last_fold_r\": 0.7299311148945875,\n  \"best_iter_A\": null,\n  \"best_iter_B\": null,\n  \"seeds\": [\n    29,\n    42,\n    73\n  ],\n  \"blend_w_global\": 0.7500000000000002,\n  \"regime_feature\": \"rv30s\",\n  \"regime_threshold\": 9.532938364212052e-05,\n  \"w_low\": 0.85,\n  \"w_high\": 0.2,\n  \"smoothing_window\": 1,\n  \"n_features\": 30,\n  \"python\": \"3.11.13\",\n  \"platform\": \"Linux-6.6.56+-x86_64-with-glibc2.35\"\n}\n\nSubmission:\nshape: (270548, 2) | columns: ['timestamp', 'labels']\nNaNs: 0 | zeros: 0\nstats: min/mean/median/max/std = 3.041559963713085e-05 7.609331111537332e-05 7.816108537324519e-05 0.0001005195357726 6.914787113803533e-06\nquantiles: {0.01: 5.1753424667172586e-05, 0.25: 7.352792990182659e-05, 0.5: 7.816108537324519e-05, 0.75: 7.966036743068411e-05, 0.99: 9.140879036073209e-05}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 900x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":26}]}
